{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP is different than DP\n",
    "# DDP, each GPU (woker) has a copy of the model\n",
    "# data is divided into batches\n",
    "# each GPU gets its batch and do the forwrad pass\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import tempfile\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(world_size, rank):\n",
    "\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"75872\"\n",
    "\n",
    "    dist.init_process_group(\"nccl\", world_size=world_size , rank= rank)\n",
    "\n",
    "def cleanup():\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel,self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu1(self.net1(x)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adamax(model.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(logits, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backwards() # this is gradinets calcilations\n",
    "optimizer.step() # this is weight upodate\n",
    "optimizer.zero_grad() # this is clearinng optimzier states for iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "from torch.optim import adamw\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DPP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple two linear feed forward network, it takes an input and passes \n",
    "# it through a layer with 10 hidden units \n",
    "# then it pass that through relu\n",
    "# then it passes it through another hidden layer which gives 5 dimensional vecto\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.net1 = nn.Linear(10,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up DDP, so that each process know the address of a single machine for all reduce\n",
    "# also we need to define world size and rank.\n",
    "\n",
    "def ddp_setup(rank: int, world_size: int) -> None:\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"  # or this should be the first node IP.\n",
    "    os.environ[\"MASTER_PORT\"] = \"any port\" # any available port\n",
    "\n",
    "\n",
    "    dist.init_process_group(\"nccl\", rank = rank, world_size= world_size)\n",
    "    torch.cuda.set_device(rank) # this ensures that gpu0 gets rank 0 and so on.\n",
    "\n",
    "\n",
    "def ddp_cleanup() -> None:\n",
    "    \"\"\"\n",
    "    This is to delete the ddp setup after everything is done.\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import TensorDataset, DistributedSampler, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "def run_epoch(rank:int, world_size: int) -> None:\n",
    "\n",
    "    print(f\"running rank {rank} and world size {world_size}\")\n",
    "\n",
    "    ddp_setup(rank, world_size) # getting \n",
    "\n",
    "    model = ToyModel().to(rank) # put the model on specific rank gpu\n",
    "    model = DDP(model, device_ids = [rank]) # Wrap the model with DDP \n",
    "    # means that each paramters hook will be automatically initiated\n",
    "    # and when the backward pass happens, for each process \n",
    "    # seperetely, when there is time of updatuing weights for each parameter\n",
    "    # of each process, then the average ghradianet will be brought for that\n",
    "    # parameter and it will be updated with that average gradient.\n",
    "    # \n",
    "\n",
    "    # synthetic data random 1000 rows\n",
    "    x = torch.randn(1000, 10)\n",
    "    y = torch.randn(1000, 5)\n",
    "\n",
    "    # encapsulate x and y in input output target pairs\n",
    "    ds = TensorDataset(x, y)\n",
    "    # Once we have the TensorDataset, we can use it with data loader ot distrbiuted data sampler\n",
    "    # DistributedDataSampler makes sure that each batch is unique\n",
    "    sampler = DistributedSampler(ds, num_replicas= world_size, rank = rank, \n",
    "                                 shuffle= True)\n",
    "    \n",
    "    # the way it works, x, y-->TensorDataset-->DistributedSampler-->DataLoader\n",
    "\n",
    "    dl = DataLoader(ds, sampler = sampler, batch_size=64)\n",
    "\n",
    "    ## the data is set now, \n",
    "    # ddp setup --> model --> model wrap to DDP--> data setup  (xy, ds, dl)\n",
    "    optimizer = AdamW(model.parameters(), lr = 1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Get exatly one epoch\n",
    "    sampler.set_epoch(0)\n",
    "\n",
    "    batch_x, batch_y = next(iter(dl))\n",
    "    batch_x = batch_x.to(rank)\n",
    "    batch_y = batch_y.to(rank)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(batch_x)\n",
    "    loss = loss_fn(input=predictions, target = batch_y)\n",
    "\n",
    "    loss.backwards() # this is where hooks are fired and regisreed. gradoients \n",
    "    # are calculated \n",
    "    optimizer.step() # this is where the local gradients for each param in each rank us replaced\n",
    "    # by the average all reduce operraion gradinet\n",
    "    print(f\" [rank {rank}] has a loss of {loss.item():.6f}\", flush=True)\n",
    "    ddp_cleanup()\n",
    "\n",
    "    print(f\"rank {rank} finished\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import  load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[\"train\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(raw[\"train\"]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens = raw.map(tokenization, batched = True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds_tokens[\"train\"][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"text\" if \"text\" in raw[\"train\"].column_names else list(raw[\"train\"].column_names)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns=[c for c in raw[\"train\"].column_names if c != \"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens.set_format(type= \"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[\"train\"].column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the first is text, then fine make that your input text otherwise the first one should be selected\n",
    "column = \"text\" if \"text\" in raw[\"train\"].column_names else raw[\"train\"].column_names[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    \n",
    "    return tokenizer(example[column], truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "token_ds = raw.map(tokenization, batched=True, remove_columns=[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors= \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheular.StepLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          DataCollatorWithPadding)\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "import os\n",
    "from torch.amp import autocast\n",
    "import contextlib\n",
    "from torch.utils.data import TensorDataset, DistributedSampler, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup(rank:int, world_size:int)->None:\n",
    "\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "def ddp_cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "raw_data = load_dataset(\"ag_news\")\n",
    "\n",
    "column = \"text\" if \"text\" in raw_data[\"train\"].column_names else raw_data[\"train\"].column_names[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "\n",
    "    tokenizer(example[column], truncation= True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds = raw_data.map(tokenization, batched=True, remove_columns=[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(token_ds[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual way \n",
    "# x = torch.randn(10, 10)\n",
    "# y = torch.randn(10, 5)\n",
    "\n",
    "# ds = TensorDataset(x , y)\n",
    "\n",
    "# sampler = DistributedSampler( dataset=ds, num_replicas=world_size, rank = rank, shuffle=True)\n",
    "\n",
    "# dl = DataLoader(dataset=ds, sampler=sampler, batch_size=32, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "# this will make sure we have padding across each batch and not the whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the ds already\n",
    "token_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenization(example, column):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    return tokenizer(example[column])\n",
    "\n",
    "\n",
    "def data_loader(rank:int, world_size:int, batch_size: int):\n",
    "\n",
    "    raw = load_dataset(\"ag_news\")\n",
    "    column = \"text\" if \"text\" in raw[\"train\"].column_names else raw[\"train\"].column_names[0]\n",
    "\n",
    "    token_ds = raw.map(tokenization, batched=True, remove_columns=[column])\n",
    "\n",
    "    token_ds.set_format(type=\"torch\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sampler_train = DistributedSampler(dataset=token_ds[\"train\"], num_replicas=world_size, \n",
    "                                   rank=rank, shuffle=True, drop_last=True)\n",
    "\n",
    "sampler_test = DistributedSampler(dataset=token_ds[\"test\"], num_replicas=world_size, \n",
    "                                   rank=rank, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
